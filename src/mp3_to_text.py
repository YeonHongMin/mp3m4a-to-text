"""
MP3 to Text Converter - Core Module
===================================
Converts MP3/WAV audio files to text using faster-whisper.
Completely free and runs locally (offline).

Usage:
    python mp3_to_text.py                    # Interactive mode
    python mp3_to_text.py audio.mp3          # CLI mode
    python mp3_to_text.py audio.mp3 -o out.txt  # Save to file
"""

import os
import sys
import argparse
import warnings
from pathlib import Path

# Suppress warnings for cleaner output
warnings.filterwarnings("ignore")

# Optional: Progress bar
try:
    from tqdm import tqdm
    TQDM_AVAILABLE = True
except ImportError:
    TQDM_AVAILABLE = False

# Check if faster-whisper is installed
try:
    from faster_whisper import WhisperModel
except ImportError:
    print("âŒ 'faster-whisper' ë¼ì´ë¸ŒëŸ¬ë¦¬ê°€ ì„¤ì¹˜ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.")
    print("   ì„¤ì¹˜: pip install faster-whisper")
    print("   ë˜í•œ ffmpegê°€ ì‹œìŠ¤í…œì— ì„¤ì¹˜ë˜ì–´ ìˆì–´ì•¼ í•©ë‹ˆë‹¤.")
    print("   ë˜í•œ ffmpegê°€ ì‹œìŠ¤í…œì— ì„¤ì¹˜ë˜ì–´ ìˆì–´ì•¼ í•©ë‹ˆë‹¤.")
    sys.exit(1)

# Audio preprocessing
try:
    from pydub import AudioSegment
    import tempfile
    PYDUB_AVAILABLE = True
except ImportError:
    PYDUB_AVAILABLE = False


def preprocess_audio(audio_path: str, target_sample_rate: int = 16000) -> str:
    """
    ì˜¤ë””ì˜¤ë¥¼ Whisper ìµœì  í¬ë§·ìœ¼ë¡œ ì „ì²˜ë¦¬.
    - 16kHz ìƒ˜í”Œë ˆì´íŠ¸
    - ëª¨ë…¸ ì±„ë„
    - WAV í¬ë§·
    """
    if not PYDUB_AVAILABLE:
        return audio_path
    
    try:
        print(f"ğŸ”„ ì˜¤ë””ì˜¤ ì „ì²˜ë¦¬ ì¤‘... (16kHz ëª¨ë…¸ ë³€í™˜)")
        # ì˜¤ë””ì˜¤ ë¡œë“œ
        audio = AudioSegment.from_file(audio_path)
        
        # ëª¨ë…¸ ë³€í™˜
        if audio.channels > 1:
            audio = audio.set_channels(1)
        
        # 16kHz ë¦¬ìƒ˜í”Œë§
        if audio.frame_rate != target_sample_rate:
            audio = audio.set_frame_rate(target_sample_rate)
        
        # ì„ì‹œ íŒŒì¼ ì €ì¥
        temp_file = tempfile.NamedTemporaryFile(suffix=".wav", delete=False)
        audio.export(temp_file.name, format="wav")
        temp_file.close()
        
        return temp_file.name
        
    except Exception as e:
        print(f"âš ï¸ ì „ì²˜ë¦¬ ì‹¤íŒ¨, ì›ë³¸ ì‚¬ìš©: {e}")
        return audio_path


def get_audio_duration(audio_path: str) -> float:
    """
    ì˜¤ë””ì˜¤ íŒŒì¼ì˜ ì´ ê¸¸ì´(ì´ˆ)ë¥¼ ë°˜í™˜.
    """
    if not PYDUB_AVAILABLE:
        return 0.0
    
    try:
        audio = AudioSegment.from_file(audio_path)
        return len(audio) / 1000.0  # ë°€ë¦¬ì´ˆ â†’ ì´ˆ
    except Exception as e:
        print(f"âš ï¸ ì˜¤ë””ì˜¤ ê¸¸ì´ í™•ì¸ ì‹¤íŒ¨: {e}")
        return 0.0


def format_time(seconds: float) -> str:
    """ì´ˆë¥¼ MM:SS ë˜ëŠ” HH:MM:SS í˜•ì‹ìœ¼ë¡œ ë³€í™˜."""
    if seconds < 0:
        return "--:--"
    hours = int(seconds // 3600)
    minutes = int((seconds % 3600) // 60)
    secs = int(seconds % 60)
    if hours > 0:
        return f"{hours}:{minutes:02d}:{secs:02d}"
    return f"{minutes:02d}:{secs:02d}"


class MP3ToTextConverter:
    """
    MP3/WAV íŒŒì¼ì„ í…ìŠ¤íŠ¸ë¡œ ë³€í™˜í•˜ëŠ” í´ë˜ìŠ¤.
    
    Attributes:
        model_size: ëª¨ë¸ í¬ê¸° ('small', 'medium', 'large', 'large-v3')
        device: ì‹¤í–‰ ì¥ì¹˜ ('cuda' or 'cpu')
        language: ë³€í™˜ ì–¸ì–´ (ê¸°ë³¸ê°’: 'ko' í•œêµ­ì–´)
    """
    
    # ëª¨ë¸ í¬ê¸°ë³„ íŠ¹ì„±
    MODEL_INFO = {
        "small": {"size": "~250MB", "speed": "ë¹ ë¦„", "accuracy": "ì–‘í˜¸"},
        "medium": {"size": "~750MB", "speed": "ë³´í†µ", "accuracy": "ì¢‹ìŒ"},
        "large": {"size": "~1.5GB", "speed": "ëŠë¦¼", "accuracy": "ë§¤ìš° ì¢‹ìŒ"},
        "large-v3": {"size": "~3GB", "speed": "ê°€ì¥ ëŠë¦¼", "accuracy": "ìµœê³  (í•œêµ­ì–´ ì¶”ì²œ)"},
    }
    
    def __init__(self, model_size: str = "large-v3", device: str = "auto", 
                 language: str = "ko", use_vad: bool = True, use_context: bool = False):
        """
        ë³€í™˜ê¸° ì´ˆê¸°í™”.
        
        Args:
            model_size: ëª¨ë¸ í¬ê¸° (small, medium, large, large-v3)
            device: 'cuda', 'cpu', ë˜ëŠ” 'auto' (ìë™ ê°ì§€)
            language: ë³€í™˜ ì–¸ì–´ ì½”ë“œ (ko, en, ja ë“±)
            use_vad: VAD í•„í„° ì‚¬ìš© ì—¬ë¶€ (ê¸°ë³¸: True, ìŒì„± ëˆ„ë½ ì‹œ False)
            use_context: ì´ì „ ë¬¸ë§¥ ê¸°ë°˜ ì¶”ë¡  ì—¬ë¶€ (ê¸°ë³¸: False, ì†ë„ ìš°ì„ )
        """
        self.model_size = model_size
        self.language = language
        self.use_vad = use_vad
        self.use_context = use_context
        
        # ìë™ ì¥ì¹˜ ê°ì§€
        if device == "auto":
            try:
                import torch
                self.device = "cuda" if torch.cuda.is_available() else "cpu"
            except ImportError:
                self.device = "cpu"
        else:
            self.device = device
        
        # ì¥ì¹˜ì— ë”°ë¥¸ ìµœì  ê³„ì‚° íƒ€ì…
        self.compute_type = "float16" if self.device == "cuda" else "int8"
        
        vad_status = "í™œì„±í™”" if use_vad else "ë¹„í™œì„±í™”"
        print(f"ğŸ”§ ì„¤ì •: ëª¨ë¸={model_size}, ì¥ì¹˜={self.device}, ì–¸ì–´={language}, VAD={vad_status}")
        print(f"ğŸ“¥ ëª¨ë¸ ë¡œë”© ì¤‘... (ì²« ì‹¤í–‰ ì‹œ ë‹¤ìš´ë¡œë“œê°€ í•„ìš”í•©ë‹ˆë‹¤)")
        
        self.model = WhisperModel(
            model_size,
            device=self.device,
            compute_type=self.compute_type
        )
        
        print(f"âœ… ëª¨ë¸ ë¡œë”© ì™„ë£Œ!")
    
    # í™˜ê°(Hallucination)ìœ¼ë¡œ ìì£¼ ë“±ì¥í•˜ëŠ” íŒ¨í„´ë“¤
    HALLUCINATION_PATTERNS = [
        "í•œê¸€ìë§‰", "ìë§‰ ì œì‘", "ìë§‰ by", "ìˆ˜ê³ í•˜ì…¨ìŠµë‹ˆë‹¤", 
        "ì‹œì²­í•´ì£¼ì…”ì„œ ê°ì‚¬í•©ë‹ˆë‹¤", "MBC", "êµ¬ë…ê³¼ ì¢‹ì•„ìš”", 
        "ì˜ìƒ í¸ì§‘", "ì œì‘ ì§€ì›", "ë²ˆì—­ :", "ì‹±í¬ :", "ë°°ê¸‰ :",
        "í•œê¸€ ìë§‰", "by í•œíš¨ì •", "í•œê¸€ìë§‰ by í•œíš¨ì •"
    ]

    def is_hallucination(self, text: str) -> bool:
        """í…ìŠ¤íŠ¸ê°€ í™˜ê°ì¸ì§€ íŒë³„"""
        if not text or len(text.strip()) == 0:
            return True
            
        # 1. ë°˜ë³µ íŒ¨í„´ ì²´í¬ (ì˜ˆ: "...." ë˜ëŠ” "??" ë°˜ë³µ)
        if len(text) > 10 and len(set(text)) < 5:
            return True
            
        # 2. ì•Œë ¤ì§„ í™˜ê° ë¬¸êµ¬ ì²´í¬
        for pattern in self.HALLUCINATION_PATTERNS:
            if pattern in text:
                return True
                
        return False

    def _transcribe_generator(self, audio_path: str, show_progress: bool = True):
        """
        ì˜¤ë””ì˜¤ íŒŒì¼ì„ ë³€í™˜í•˜ëŠ” ì œë„ˆë ˆì´í„° (ìŠ¤íŠ¸ë¦¬ë°ìš©).
        Yields:
            (segment, info, total_duration, processed_path)
        """
        if not os.path.exists(audio_path):
            raise FileNotFoundError(f"íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {audio_path}")
        
        # ì˜¤ë””ì˜¤ ê¸¸ì´ í™•ì¸
        total_duration = get_audio_duration(audio_path)
        if total_duration > 0:
            print(f"ğŸµ ë³€í™˜ ì¤‘: {audio_path} (ì´ {format_time(total_duration)})")
        else:
            print(f"ğŸµ ë³€í™˜ ì¤‘: {audio_path}")
        
        # ì˜¤ë””ì˜¤ ì „ì²˜ë¦¬ ìˆ˜í–‰
        processed_path = preprocess_audio(audio_path)
        process_audio_path = processed_path if processed_path != audio_path else audio_path
        
        # ì„¤ì • ê°€ì ¸ì˜¤ê¸°
        use_vad = getattr(self, 'use_vad', True)
        use_context = getattr(self, 'use_context', False)
        
        # í•œêµ­ì–´ ì¸ì‹ ìœ ë„ë¥¼ ìœ„í•œ í”„ë¡¬í”„íŠ¸ (í™˜ê° ë°©ì§€ìš©)
        initial_prompt = "[í•œê¸€ ìŒì„± ì¶”ì¶œ]" if self.language == 'ko' else None

        if use_vad:
            segments, info = self.model.transcribe(
                process_audio_path,
                language=self.language,
                beam_size=5,
                condition_on_previous_text=use_context,
                temperature=0,  # ë°˜ë³µ íƒìƒ‰ ë°©ì§€
                initial_prompt=initial_prompt,
                vad_filter=True,
                vad_parameters=dict(threshold=0.05, min_speech_duration_ms=50, min_silence_duration_ms=50),
            )
        else:
            print("âš ï¸ VAD ë¹„í™œì„±í™”: ì „ì²´ ì˜¤ë””ì˜¤ ì²˜ë¦¬")
            segments, info = self.model.transcribe(
                process_audio_path,
                language=self.language,
                beam_size=5,
                condition_on_previous_text=use_context,
                temperature=0,
                initial_prompt=initial_prompt,
                vad_filter=False,
            )
        
        if show_progress:
            print("ğŸ“Š ì‹¤ì‹œê°„ ì§„í–‰ ìƒí™©:")
            print("   â³ ëª¨ë¸ ì²˜ë¦¬ ì¤‘... (ì²« ê²°ê³¼ê¹Œì§€ ì ì‹œ ëŒ€ê¸°)", end="", flush=True)
        
        first_segment = True
        for segment in segments:
            # í™˜ê° í•„í„°ë§
            if self.is_hallucination(segment.text):
                continue
            
            # ì²« ì„¸ê·¸ë¨¼íŠ¸ ì‹œ ëŒ€ê¸° ë©”ì‹œì§€ ì§€ìš°ê¸°
            if first_segment and show_progress:
                print("\r" + " " * 50 + "\r", end="", flush=True)
                first_segment = False
                
            yield segment, info, total_duration, processed_path

    def transcribe(self, audio_path: str, show_timestamps: bool = False, 
                   show_progress: bool = True) -> dict:
        """
        ì˜¤ë””ì˜¤ íŒŒì¼ì„ í…ìŠ¤íŠ¸ë¡œ ë³€í™˜.
        
        Args:
            audio_path: MP3/WAV íŒŒì¼ ê²½ë¡œ
            show_timestamps: íƒ€ì„ìŠ¤íƒ¬í”„ í‘œì‹œ ì—¬ë¶€
            show_progress: ì§„í–‰ ìƒí™© ì‹¤ì‹œê°„ í‘œì‹œ ì—¬ë¶€
            
        Returns:
            dict: {
                'text': ì „ì²´ í…ìŠ¤íŠ¸,
                'language': ê°ì§€ëœ ì–¸ì–´,
                'language_probability': ì–¸ì–´ ê°ì§€ í™•ë¥ ,
                'segments': ì„¸ê·¸ë¨¼íŠ¸ ëª©ë¡ (íƒ€ì„ìŠ¤íƒ¬í”„ í¬í•¨)
            }
        """
        segment_list = []
        full_text_parts = []
        info = None
        segment_count = 0
        processed_path = audio_path # Default, will be updated by generator
        
        import time as time_module
        start_time = time_module.time()
        
        for segment, info_obj, total_duration, gen_processed_path in self._transcribe_generator(audio_path, show_progress):
            if info is None: info = info_obj
            if processed_path == audio_path: processed_path = gen_processed_path # Capture processed_path once
            
            segment_count += 1
            segment_list.append({
                "start": segment.start,
                "end": segment.end,
                "text": segment.text
            })
            full_text_parts.append(segment.text)
            
            # ì§„í–‰ ìƒí™© í‘œì‹œ
            if show_progress:
                current_audio_time = segment.end
                elapsed_real_time = time_module.time() - start_time
                
                if total_duration > 0:
                    # ì§„í–‰ë¥  ê³„ì‚°
                    progress_pct = min(100, (current_audio_time / total_duration) * 100)
                    
                    # ETA ê³„ì‚° (í˜„ì¬ ì†ë„ ê¸°ì¤€)
                    if current_audio_time > 0:
                        speed_ratio = elapsed_real_time / current_audio_time
                        remaining_audio = total_duration - current_audio_time
                        eta_seconds = remaining_audio * speed_ratio
                    else:
                        eta_seconds = -1
                    
                    # ì§„í–‰ ë°” í‘œì‹œ
                    bar_width = 20
                    filled = int(bar_width * progress_pct / 100)
                    bar = "â–ˆ" * filled + "â–‘" * (bar_width - filled)
                    
                    # í…ìŠ¤íŠ¸ ë¯¸ë¦¬ë³´ê¸° (30ì ì œí•œ)
                    text_preview = segment.text[:25] + "..." if len(segment.text) > 25 else segment.text
                    
                    # ìƒíƒœ ë¼ì¸ ì¶œë ¥ (ê°™ì€ ì¤„ ë®ì–´ì“°ê¸°)
                    status = f"\r[{bar}] {progress_pct:5.1f}% | {format_time(current_audio_time)}/{format_time(total_duration)} | ETA: {format_time(eta_seconds)} | {text_preview}"
                    print(status.ljust(120), end="", flush=True)
                else:
                    # ì˜¤ë””ì˜¤ ê¸¸ì´ë¥¼ ëª¨ë¥¼ ë•ŒëŠ” ì„¸ê·¸ë¨¼íŠ¸ ìˆ˜ì™€ ì²˜ë¦¬ ì‹œê°„ë§Œ í‘œì‹œ
                    text_preview = segment.text[:30] + "..." if len(segment.text) > 30 else segment.text
                    print(f"\rì„¸ê·¸ë¨¼íŠ¸ {segment_count} | {format_time(current_audio_time)} | {text_preview}".ljust(100), end="", flush=True)
            
            if show_timestamps:
                print(f"\n  [{segment.start:.2f}s â†’ {segment.end:.2f}s] {segment.text}")
        
        # ì§„í–‰ í‘œì‹œ ì¢…ë£Œ (ì¤„ë°”ê¿ˆ)
        if show_progress:
            print()  # ìƒˆ ì¤„ë¡œ ì´ë™
            
        # ì„ì‹œ íŒŒì¼ ì •ë¦¬
        if processed_path != audio_path and os.path.exists(processed_path):
            try:
                os.unlink(processed_path)
            except:
                pass
        
        full_text = " ".join(full_text_parts).strip()
        
        result = {
            "text": full_text,
            "language": info.language,
            "language_probability": info.language_probability,
            "segments": segment_list
        }
        
        print(f"\nâœ… ë³€í™˜ ì™„ë£Œ! (ì´ {segment_count}ê°œ ì„¸ê·¸ë¨¼íŠ¸, ê°ì§€ ì–¸ì–´: {info.language}, í™•ë¥ : {info.language_probability:.2%})")
        
        return result
    
    def transcribe_to_file(self, audio_path: str, output_path: str, 
                           include_timestamps: bool = False) -> str:
        """
        ì˜¤ë””ì˜¤ íŒŒì¼ì„ í…ìŠ¤íŠ¸ë¡œ ë³€í™˜í•˜ê³  íŒŒì¼ë¡œ ì €ì¥.
        
        Args:
            audio_path: ì…ë ¥ ì˜¤ë””ì˜¤ íŒŒì¼ ê²½ë¡œ
            output_path: ì¶œë ¥ í…ìŠ¤íŠ¸ íŒŒì¼ ê²½ë¡œ
            include_timestamps: íƒ€ì„ìŠ¤íƒ¬í”„ í¬í•¨ ì—¬ë¶€
            
        Returns:
            str: ì €ì¥ëœ íŒŒì¼ ê²½ë¡œ
        """
        result = self.transcribe(audio_path, show_timestamps=False)
        
        with open(output_path, "w", encoding="utf-8") as f:
            f.write(f"# Audio Transcription\n")
            f.write(f"# Source: {audio_path}\n")
            f.write(f"# Language: {result['language']} ({result['language_probability']:.2%})\n")
            f.write(f"# ---\n\n")
            
            if include_timestamps:
                for seg in result['segments']:
                    f.write(f"[{seg['start']:.2f}s - {seg['end']:.2f}s]\n")
                    f.write(f"{seg['text']}\n\n")
            else:
                f.write(result['text'])
        
        print(f"ğŸ“„ ê²°ê³¼ ì €ì¥ë¨: {output_path}")
        
        try:
            for segment, info_obj, total_duration, _ in self._transcribe_generator(audio_path, show_progress=True):
                if info is None:
                    info = info_obj
                    # ì–¸ì–´ ì •ë³´ ë“± íŒŒì¼ì— ì—…ë°ì´íŠ¸ (ì„ íƒ ì‚¬í•­, ë³µì¡í•´ì§€ë¯€ë¡œ ìƒëµí•˜ê±°ë‚˜ ë‚˜ì¤‘ì— ì¶”ê°€)

                # 1. ì „ì²´ í…ìŠ¤íŠ¸ íŒŒì¼ì— ì¶”ê°€ (Append)
                with open(full_file, "a", encoding="utf-8") as f:
                    text_chunk = segment.text.strip()
                    if text_chunk:
                        # ë¬¸ì¥ ëì— ë§ˆì¹¨í‘œê°€ ìˆìœ¼ë©´ ì¤„ë°”ê¿ˆ
                        if text_chunk.endswith('.'):
                            f.write(f"{text_chunk}\n\n")
                        else:
                            f.write(f"{text_chunk} ")

                # 2. ì‹œê°„ êµ¬ê°„ íŒŒì¼ì— ì¶”ê°€ (Append)
                with open(time_file, "a", encoding="utf-8") as f:
                    start_str = format_time(segment.start)
                    # end_str = format_time(segment.end) # í•„ìš”í•œ ê²½ìš° ì‚¬ìš©
                    f.write(f"| {start_str} | {segment.text.strip()} |\n")

        except KeyboardInterrupt:
            print("\nğŸ›‘ ì‚¬ìš©ìì— ì˜í•´ ì¤‘ë‹¨ë˜ì—ˆìŠµë‹ˆë‹¤. í˜„ì¬ê¹Œì§€ì˜ ê²°ê³¼ëŠ” ì €ì¥ë˜ì—ˆìŠµë‹ˆë‹¤.")
            with open(full_file, "a", encoding="utf-8") as f:
                f.write("\n\n> **âš ï¸ ì¤‘ë‹¨ë¨: ì‚¬ìš©ìì— ì˜í•´ ì‘ì—…ì´ ì·¨ì†Œë˜ì—ˆìŠµë‹ˆë‹¤.**\n")
            with open(time_file, "a", encoding="utf-8") as f:
                f.write("\n> **âš ï¸ ì¤‘ë‹¨ë¨: ì‚¬ìš©ìì— ì˜í•´ ì‘ì—…ì´ ì·¨ì†Œë˜ì—ˆìŠµë‹ˆë‹¤.**\n")
            return time_file, full_file, log_file

        # ì¢…ë£Œ ì²˜ë¦¬
        end_time = time.time()
        elapsed_str = format_time(end_time - start_time)
        
        # ë¡œê·¸ì— ê²°ê³¼ ì—…ë°ì´íŠ¸
        with open(log_file, "a", encoding="utf-8") as f:
            if info:
                f.write(f"| **ì–¸ì–´** | {info.language} ({info.language_probability:.1%}) |\n")
            f.write(f"| **ì†Œìš” ì‹œê°„** | {elapsed_str} |\n\n")

        # íŒŒì¼ ìƒë‹¨ ì •ë³´ ì—…ë°ì´íŠ¸ (ì„ íƒì : íŒŒì¼ì„ ë‹¤ì‹œ ì½ì–´ì„œ í—¤ë” ìˆ˜ì •ì€ ë³µì¡í•˜ë¯€ë¡œ ê¼¬ë¦¬ë§ ì¶”ê°€)
        with open(full_file, "a", encoding="utf-8") as f:
            f.write(f"\n\n---\nâœ… **ë³€í™˜ ì™„ë£Œ** (ì†Œìš” ì‹œê°„: {elapsed_str})")
            
        with open(time_file, "a", encoding="utf-8") as f:
            f.write(f"\n\n---\nâœ… **ë³€í™˜ ì™„ë£Œ** (ì†Œìš” ì‹œê°„: {elapsed_str})")

        print(f"\nâœ… ë³€í™˜ ì™„ë£Œ! (ì´ {elapsed_str})")
        print(f"ğŸ“„ ì „ì²´ ë‚´ìš©: {full_file}")
        print(f"ğŸ“„ ì‹œê°„ êµ¬ê°„: {time_file}")
        
        return time_file, full_file, log_file
    
    def transcribe_to_files(self, audio_path: str, output_base: str, 
                            time_interval: int = 30) -> tuple:
        """
        ì˜¤ë””ì˜¤ íŒŒì¼ì„ í…ìŠ¤íŠ¸ë¡œ ë³€í™˜í•˜ê³  ë‘ ê°€ì§€ í˜•ì‹ìœ¼ë¡œ ì €ì¥ (ì‹¤ì‹œê°„ ê¸°ë¡).
        """
        import time
        from datetime import datetime
        
        # ì‹œì‘ ì‹œê°„ ê¸°ë¡
        start_time = time.time()
        start_datetime = datetime.now().strftime("%Y-%m-%d %H:%M:%S")
        date_str = datetime.now().strftime("%Y-%m-%d")
        
        # íŒŒì¼ ê²½ë¡œ ìƒì„±
        time_file = f"{output_base}_time.md"
        full_file = f"{output_base}_full.md"
        
        # ë¡œê·¸ íŒŒì¼ ì¤€ë¹„
        log_dir = os.path.join(os.path.dirname(output_base) or ".", "log")
        os.makedirs(log_dir, exist_ok=True)
        log_file = os.path.join(log_dir, f"{date_str}.md")
        
        # ë¡œê·¸ íŒŒì¼ í—¤ë” (append)
        is_new_log = not os.path.exists(log_file) or os.path.getsize(log_file) == 0
        with open(log_file, "a", encoding="utf-8") as f:
            if is_new_log:
                f.write(f"# ğŸ“‹ Transcription Log - {date_str}\n\n")
            f.write(f"---\n\n")
            f.write(f"## ğŸµ {os.path.basename(audio_path)}\n\n")
            f.write(f"| í•­ëª© | ê°’ |\n|---|---|\n")
            f.write(f"| **íŒŒì¼** | `{audio_path}` |\n")
            f.write(f"| **ì‹œì‘ ì‹œê°„** | {start_datetime} |\n")
            f.write(f"| **VAD** | {'í™œì„±í™”' if self.use_vad else 'ë¹„í™œì„±í™”'} |\n")
        
        # ì¶œë ¥ íŒŒì¼ ì´ˆê¸°í™”
        with open(full_file, "w", encoding="utf-8") as f:
            f.write(f"# ğŸ“ Audio Transcription - Full Text\n\n")
            f.write(f"> **íŒŒì¼**: `{audio_path}`  \n")
            f.write(f"> **ìƒíƒœ**: ë³€í™˜ ì¤‘... (ì‹¤ì‹œê°„ ì—…ë°ì´íŠ¸)\n\n---\n\n")
            
        with open(time_file, "w", encoding="utf-8") as f:
            f.write(f"# â±ï¸ Audio Transcription - Time Intervals\n\n")
            f.write(f"> **íŒŒì¼**: `{audio_path}`  \n")
            f.write(f"> **ìƒíƒœ**: ë³€í™˜ ì¤‘... (ì‹¤ì‹œê°„ ì—…ë°ì´íŠ¸)\n\n---\n\n")
            f.write(f"| ì‹œê°„ | ë‚´ìš© |\n|---|---|\n")

        # ì‹¤ì‹œê°„ ë³€í™˜ ë° ì €ì¥
        info = None
        logged_progress = set()  # ì´ë¯¸ ë¡œê·¸ì— ê¸°ë¡í•œ ì§„í–‰ë¥ 
        segment_count = 0
        
        try:
            for segment, info_obj, total_duration, _ in self._transcribe_generator(audio_path, show_progress=True):
                segment_count += 1
                
                if info is None:
                    info = info_obj
                    # ì–¸ì–´ ì •ë³´ ë¡œê·¸ì— ì—…ë°ì´íŠ¸
                    with open(log_file, "a", encoding="utf-8") as f:
                         f.write(f"| **ì–¸ì–´** | {info.language} ({info.language_probability:.1%}) |\n")

                # ì§„í–‰ë¥  ê³„ì‚°
                if total_duration > 0:
                    progress = segment.end / total_duration * 100
                    progress_int = int(progress)
                    elapsed = time.time() - start_time
                    
                    # ETA ê³„ì‚°
                    if progress > 0:
                        eta = elapsed * (100 - progress) / progress
                        eta_str = format_time(eta)
                    else:
                        eta_str = "?"
                    
                    # ì½˜ì†”ì— ì§„í–‰ë¥  ë°” ì¶œë ¥
                    bar_length = 20
                    filled = int(bar_length * progress / 100)
                    bar = "â–ˆ" * filled + "â–‘" * (bar_length - filled)
                    time_info = f"{format_time(segment.end)}/{format_time(total_duration)}"
                    text_preview = segment.text.strip()[:25]
                    print(f"\r[{bar}] {progress:5.1f}% | {time_info} | ETA: {eta_str} | {text_preview:<25}", end="", flush=True)
                    
                    # ë¡œê·¸ ê¸°ë¡ (10% ë‹¨ìœ„)
                    for milestone in range(10, 100, 10):
                        if progress_int >= milestone and milestone not in logged_progress:
                            logged_progress.add(milestone)
                            with open(log_file, "a", encoding="utf-8") as f:
                                f.write(f"| **ì§„í–‰ë¥ ** | {milestone}% ({format_time(segment.end)}/{format_time(total_duration)}) |\n")
                else:
                    # total_durationì„ ëª¨ë¥¼ ë•Œ
                    print(f"\rì„¸ê·¸ë¨¼íŠ¸ {segment_count} | {format_time(segment.end)} | {segment.text.strip()[:30]:<30}", end="", flush=True)

                # 1. ì „ì²´ í…ìŠ¤íŠ¸ íŒŒì¼ì— ì¶”ê°€ (Append)
                with open(full_file, "a", encoding="utf-8") as f:
                    text_chunk = segment.text.strip()
                    if text_chunk:
                        if text_chunk.endswith('.'):
                            f.write(f"{text_chunk}\n\n")
                        else:
                            f.write(f"{text_chunk} ")

                # 2. ì‹œê°„ êµ¬ê°„ íŒŒì¼ì— ì¶”ê°€ (Append)
                with open(time_file, "a", encoding="utf-8") as f:
                    start_str = format_time(segment.start)
                    f.write(f"| {start_str} | {segment.text.strip()} |\n")

        except KeyboardInterrupt:
            print("\nğŸ›‘ ì‚¬ìš©ìì— ì˜í•´ ì¤‘ë‹¨ë˜ì—ˆìŠµë‹ˆë‹¤. í˜„ì¬ê¹Œì§€ì˜ ê²°ê³¼ëŠ” ì €ì¥ë˜ì—ˆìŠµë‹ˆë‹¤.")
            with open(full_file, "a", encoding="utf-8") as f:
                f.write("\n\n> **âš ï¸ ì¤‘ë‹¨ë¨: ì‚¬ìš©ìì— ì˜í•´ ì‘ì—…ì´ ì·¨ì†Œë˜ì—ˆìŠµë‹ˆë‹¤.**\n")
            with open(time_file, "a", encoding="utf-8") as f:
                f.write("\n> **âš ï¸ ì¤‘ë‹¨ë¨: ì‚¬ìš©ìì— ì˜í•´ ì‘ì—…ì´ ì·¨ì†Œë˜ì—ˆìŠµë‹ˆë‹¤.**\n")
            return time_file, full_file, log_file

        # ì¢…ë£Œ ì²˜ë¦¬
        end_time = time.time()
        elapsed_str = format_time(end_time - start_time)
        
        # ë¡œê·¸ì— ê²°ê³¼ ì—…ë°ì´íŠ¸
        with open(log_file, "a", encoding="utf-8") as f:
            f.write(f"| **ì†Œìš” ì‹œê°„** | {elapsed_str} |\n\n")

        # íŒŒì¼ ìƒë‹¨ ì •ë³´ ì—…ë°ì´íŠ¸ (ê¼¬ë¦¬ë§ ì¶”ê°€)
        with open(full_file, "a", encoding="utf-8") as f:
            f.write(f"\n\n---\nâœ… **ë³€í™˜ ì™„ë£Œ** (ì†Œìš” ì‹œê°„: {elapsed_str})")
            
        with open(time_file, "a", encoding="utf-8") as f:
            f.write(f"\n\n---\nâœ… **ë³€í™˜ ì™„ë£Œ** (ì†Œìš” ì‹œê°„: {elapsed_str})")

        print(f"\nâœ… ë³€í™˜ ì™„ë£Œ! (ì´ {elapsed_str})")
        print(f"ğŸ“„ ì „ì²´ ë‚´ìš©: {full_file}")
        print(f"ğŸ“„ ì‹œê°„ êµ¬ê°„: {time_file}")
        
        return time_file, full_file, log_file



def main():
    """CLI ì—”íŠ¸ë¦¬ í¬ì¸íŠ¸."""
    parser = argparse.ArgumentParser(
        description="MP3/WAV íŒŒì¼ì„ í…ìŠ¤íŠ¸ë¡œ ë³€í™˜í•©ë‹ˆë‹¤ (ë¬´ë£Œ, ë¡œì»¬ ì‹¤í–‰)",
        formatter_class=argparse.RawDescriptionHelpFormatter,
        epilog="""
ì˜ˆì‹œ:
  python mp3_to_text.py audio.mp3                    # ê¸°ë³¸ ë³€í™˜
  python mp3_to_text.py audio.mp3 -o result.txt      # íŒŒì¼ë¡œ ì €ì¥
  python mp3_to_text.py audio.mp3 -o result --dual   # ë‘ ê°€ì§€ ë²„ì „ ì €ì¥ (_time.md, _full.md)
  python mp3_to_text.py audio.mp3 -m large-v3        # ìµœê³  ì •í™•ë„ ëª¨ë¸
  python mp3_to_text.py audio.mp3 -t                 # íƒ€ì„ìŠ¤íƒ¬í”„ í‘œì‹œ
  python mp3_to_text.py --dir ./mp3                  # ë””ë ‰í„°ë¦¬ ë‚´ ëª¨ë“  íŒŒì¼ ì¼ê´„ ë³€í™˜
        """
    )
    
    parser.add_argument("audio_file", nargs="?", help="ë³€í™˜í•  MP3/WAV íŒŒì¼ ê²½ë¡œ")
    parser.add_argument("-o", "--output", help="ì¶œë ¥ íŒŒì¼ ê²½ë¡œ (ìƒëµ ì‹œ ì½˜ì†” ì¶œë ¥)")
    parser.add_argument("-m", "--model", default="large-v3",
                        choices=["small", "medium", "large", "large-v3"],
                        help="ëª¨ë¸ í¬ê¸° (ê¸°ë³¸: large-v3)")
    parser.add_argument("-l", "--language", default="ko",
                        help="ì–¸ì–´ ì½”ë“œ (ê¸°ë³¸: ko í•œêµ­ì–´)")
    parser.add_argument("--device", default="auto",
                        choices=["auto", "cuda", "cpu"],
                        help="ì‹¤í–‰ ì¥ì¹˜ (ê¸°ë³¸: auto)")
    parser.add_argument("-t", "--timestamps", action="store_true",
                        help="íƒ€ì„ìŠ¤íƒ¬í”„ í‘œì‹œ")
    parser.add_argument("--dual", action="store_true",
                        help="ë‘ ê°€ì§€ ë²„ì „ ì €ì¥ (_time.md, _full.md)")
    parser.add_argument("--interval", type=int, default=30,
                        help="ì‹œê°„ êµ¬ê°„ (ì´ˆ, ê¸°ë³¸: 30)")
    parser.add_argument("--no-vad", action="store_true",
                        help="VAD ë¹„í™œì„±í™” (ìŒì„± ëˆ„ë½ ë°©ì§€, ì²˜ë¦¬ ì‹œê°„ ì¦ê°€)")
    parser.add_argument("--context", action="store_true",
                        help="ì´ì „ ë¬¸ë§¥ ì°¸ì¡° í™œì„±í™” (ì •í™•ë„ í–¥ìƒ, ì†ë„ ì €í•˜ ê°€ëŠ¥)")
    parser.add_argument("--dir", metavar="DIRECTORY",
                        help="ë””ë ‰í„°ë¦¬ ë‚´ ëª¨ë“  MP3/WAV íŒŒì¼ ì¼ê´„ ë³€í™˜")
    
    args = parser.parse_args()
    
    # ë””ë ‰í„°ë¦¬ ëª¨ë“œ (--dir ì˜µì…˜)
    if args.dir:
        import glob
        
        dir_path = args.dir
        if not os.path.isdir(dir_path):
            print(f"âŒ ë””ë ‰í„°ë¦¬ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {dir_path}")
            sys.exit(1)
        
        # MP3/WAV/M4A íŒŒì¼ ê²€ìƒ‰
        audio_files = []
        for ext in ['*.mp3', '*.MP3', '*.wav', '*.WAV', '*.m4a', '*.M4A']:
            audio_files.extend(glob.glob(os.path.join(dir_path, ext)))
        
        if not audio_files:
            print(f"âš ï¸ ë””ë ‰í„°ë¦¬ì— ì˜¤ë””ì˜¤ íŒŒì¼ì´ ì—†ìŠµë‹ˆë‹¤ (mp3/wav/m4a): {dir_path}")
            sys.exit(1)
        
        audio_files.sort()
        print(f"\nğŸ“‚ ë””ë ‰í„°ë¦¬: {dir_path}")
        print(f"ğŸµ ë°œê²¬ëœ íŒŒì¼: {len(audio_files)}ê°œ")
        print("=" * 50)
        
        for i, audio_file in enumerate(audio_files):
            print(f"  {i+1}. {os.path.basename(audio_file)}")
        print("=" * 50 + "\n")
        
        # ë³€í™˜ê¸° ì´ˆê¸°í™” (1íšŒ)
        use_vad = not getattr(args, 'no_vad', False)
        use_context = getattr(args, 'context', False)
        
        converter = MP3ToTextConverter(
            model_size=args.model,
            device=args.device,
            language=args.language,
            use_vad=use_vad,
            use_context=use_context
        )
        
        # ê° íŒŒì¼ ë³€í™˜
        success_count = 0
        fail_count = 0
        
        for i, audio_file in enumerate(audio_files):
            print(f"\n[{i+1}/{len(audio_files)}] ì²˜ë¦¬ ì¤‘: {os.path.basename(audio_file)}")
            print("-" * 50)
            
            try:
                # ì¶œë ¥ íŒŒì¼ ê²½ë¡œ ìƒì„± (í™•ì¥ì ì œê±°)
                base_name = os.path.splitext(audio_file)[0]
                
                converter.transcribe_to_files(
                    audio_file,
                    base_name,
                    time_interval=args.interval
                )
                success_count += 1
                
            except Exception as e:
                print(f"âŒ ì˜¤ë¥˜: {e}")
                fail_count += 1
        
        # ìµœì¢… ìš”ì•½
        print(f"\n" + "=" * 50)
        print(f"ğŸ‰ ì¼ê´„ ë³€í™˜ ì™„ë£Œ!")
        print(f"   âœ… ì„±ê³µ: {success_count}ê°œ")
        if fail_count > 0:
            print(f"   âŒ ì‹¤íŒ¨: {fail_count}ê°œ")
        print("=" * 50)
    
    # ì¸í„°ë™í‹°ë¸Œ ëª¨ë“œ
    elif args.audio_file is None:
        print("=" * 50)
        print("ğŸ¤ MP3 â†’ í…ìŠ¤íŠ¸ ë³€í™˜ê¸° (ë¬´ë£Œ ë¡œì»¬ ì‹¤í–‰)")
        print("=" * 50)
        print("\nëª¨ë¸ í¬ê¸° ì˜µì…˜:")
        for name, info in MP3ToTextConverter.MODEL_INFO.items():
            print(f"  â€¢ {name}: {info['size']} / {info['speed']} / ì •í™•ë„: {info['accuracy']}")
        
        print("\níŒŒì¼ ê²½ë¡œë¥¼ ì…ë ¥í•˜ì„¸ìš” (ì¢…ë£Œ: q):")
        
        converter = None
        while True:
            audio_path = input("\nğŸ“ ì˜¤ë””ì˜¤ íŒŒì¼: ").strip()
            
            if audio_path.lower() == 'q':
                print("ğŸ‘‹ ì¢…ë£Œí•©ë‹ˆë‹¤.")
                break
            
            if not audio_path:
                continue
                
            # ì²¨ ë²ˆì§¸ íŒŒì¼ ì²˜ë¦¬ ì‹œ ëª¨ë¸ ë¡œë“œ
            if converter is None:
                model_choice = input("ëª¨ë¸ ì„ íƒ (small/medium/large/large-v3) [large-v3]: ").strip()
                if model_choice not in MP3ToTextConverter.MODEL_INFO:
                    model_choice = "large-v3"
                
                converter = MP3ToTextConverter(model_size=model_choice)
            
            try:
                result = converter.transcribe(audio_path, show_timestamps=args.timestamps)
                print("\nğŸ“ ë³€í™˜ ê²°ê³¼:")
                print("-" * 40)
                print(result['text'])
                print("-" * 40)
            except Exception as e:
                print(f"âŒ ì˜¤ë¥˜: {e}")
    else:
        # CLI ëª¨ë“œ (ë‹¨ì¼ íŒŒì¼)
        use_vad = not getattr(args, 'no_vad', False)
        use_context = getattr(args, 'context', False)
        
        converter = MP3ToTextConverter(
            model_size=args.model,
            device=args.device,
            language=args.language,
            use_vad=use_vad,
            use_context=use_context
        )
        
        if args.dual:
            # ë‘ ê°€ì§€ ë²„ì „ ì €ì¥ (_time.md, _full.md)
            # -o ì˜µì…˜ì´ ìˆìœ¼ë©´ ê·¸ ê°’ ì‚¬ìš©, ì—†ìœ¼ë©´ ì…ë ¥ íŒŒì¼ëª…ì—ì„œ ìë™ ìƒì„±
            if args.output:
                output_base = args.output
                if output_base.endswith('.txt') or output_base.endswith('.md'):
                    output_base = output_base[:-4] if output_base.endswith('.txt') else output_base[:-3]
            else:
                # ì…ë ¥ íŒŒì¼ëª…ì—ì„œ í™•ì¥ì ì œê±°í•˜ì—¬ ì¶œë ¥ ê²½ë¡œ ìƒì„±
                output_base = os.path.splitext(args.audio_file)[0]
            
            converter.transcribe_to_files(
                args.audio_file,
                output_base,
                time_interval=args.interval
            )
        elif args.output:
            # ë‹¨ì¼ íŒŒì¼ ì €ì¥
            converter.transcribe_to_file(
                args.audio_file, 
                args.output,
                include_timestamps=args.timestamps
            )
        else:
            result = converter.transcribe(args.audio_file, show_timestamps=args.timestamps)
            print("\nğŸ“ ë³€í™˜ ê²°ê³¼:")
            print("=" * 50)
            print(result['text'])
            print("=" * 50)


if __name__ == "__main__":
    main()
